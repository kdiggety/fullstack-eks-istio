name: Deploy Local K8s (Umbrella Helm)

on:
  push:
    branches: [ main ]
    tags: [ 'v*.*.*' ]
  workflow_dispatch:
    inputs:
      tag:
        description: "Optional image tag override (e.g., v0.1.0 or a commit SHA)"
        required: false

concurrency:
  group: deploy-local
  cancel-in-progress: true

permissions:
  contents: read
  packages: read

jobs:
  deploy:
    runs-on: [self-hosted, local-k8s]
    env:
      NAMESPACE: sample
      WEB_IMAGE: ghcr.io/${{ github.repository_owner }}/web
      API_IMAGE: ghcr.io/${{ github.repository_owner }}/api
      GHCR_USERNAME: ${{ secrets.GHCR_USERNAME }}
      GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
      PULL_SECRET_NAME: ghcr-pull

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Resolve base tag (prefer input → release tag → full SHA)
        id: base
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${{ github.event.inputs.tag }}" ]; then
            TAG="${{ github.event.inputs.tag }}"
          elif [[ "${GITHUB_REF}" == refs/tags/v* ]]; then
            TAG="${GITHUB_REF_NAME}"
          else
            TAG="${GITHUB_SHA}"
          fi
          echo "TAG=$TAG" >> "$GITHUB_OUTPUT"
          echo "Base tag: $TAG"

      - name: Show cluster context
        shell: bash
        run: |
          set -euo pipefail
          kubectl config current-context
          kubectl get nodes

      - name: Ensure namespace & sidecar injection
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns "$NAMESPACE" >/dev/null 2>&1 || kubectl create ns "$NAMESPACE"
          kubectl label ns "$NAMESPACE" istio-injection=enabled --overwrite

      - name: Login to GHCR (optional)
        if: ${{ env.GHCR_USERNAME != '' && env.GHCR_TOKEN != '' }}
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ env.GHCR_USERNAME }}
          password: ${{ env.GHCR_TOKEN }}

      - name: Ensure GHCR imagePullSecret + patch default SA (optional)
        if: ${{ env.GHCR_USERNAME != '' && env.GHCR_TOKEN != '' }}
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "$NAMESPACE" delete secret "$PULL_SECRET_NAME" --ignore-not-found
          kubectl -n "$NAMESPACE" create secret docker-registry "$PULL_SECRET_NAME" \
            --docker-server=ghcr.io \
            --docker-username="$GHCR_USERNAME" \
            --docker-password="$GHCR_TOKEN"
          kubectl -n "$NAMESPACE" patch serviceaccount default \
            --type merge \
            -p "{\"imagePullSecrets\":[{\"name\":\"${PULL_SECRET_NAME}\"}]}"

      - name: Resolve WEB_TAG and API_TAG with preflight check
        id: tags
        shell: bash
        env:
          BASE_TAG: ${{ steps.base.outputs.TAG }}
        run: |
          set -euo pipefail
          resolve() {
            local image="$1" tag="$2" fallback="$3"
            if docker buildx imagetools inspect "${image}:${tag}" >/dev/null 2>&1; then
              echo "$tag"
            else
              echo "$fallback"
            fi
          }
          WEB_TAG="$(resolve "$WEB_IMAGE" "$BASE_TAG" dev)"
          API_TAG="$(resolve "$API_IMAGE" "$BASE_TAG" dev)"
          echo "WEB_TAG=$WEB_TAG" >> "$GITHUB_OUTPUT"
          echo "API_TAG=$API_TAG" >> "$GITHUB_OUTPUT"
          echo "Using WEB_TAG=$WEB_TAG, API_TAG=$API_TAG"

      - name: Add helm repos
        shell: bash
        run: |
          set -euo pipefail
          helm repo add bitnami https://charts.bitnami.com/bitnami
          helm repo update

      - name: Normalize Helm dependencies (auto-fix Chart.lock)
        shell: bash
        run: |
          set -euo pipefail
          pushd ./helm/chart >/dev/null
      
          echo "Chart.yaml dependencies:"
          grep -A3 -n '^dependencies:' Chart.yaml || true
          echo
      
          # Clean any previously vendored charts to avoid stale tgz’s
          rm -rf charts
          mkdir -p charts
      
          # If Chart.lock is out of sync with Chart.yaml, delete it and rebuild fresh.
          # We let 'helm dependency build' be the oracle: if it fails, the lock is stale.
          if [ -f Chart.lock ]; then
            if ! helm dependency build . >/dev/null 2>&1; then
              echo "Chart.lock is stale → removing and regenerating..."
              rm -f Chart.lock
            else
              echo "Chart.lock appears in sync (pre-check)."
            fi
          else
            echo "No Chart.lock present (will be generated)."
          fi
      
          # Always ensure repos are up to date (covers remote deps like bitnami/redis)
          helm repo update
      
          # Update will (re)write Chart.lock to match Chart.yaml and fetch file:// deps
          helm dependency update .
      
          # Build will vendor exact versions into ./charts (reproducible installs)
          helm dependency build .
      
          echo
          echo "Final dependency tree:"
          ls -l charts || true
      
          popd >/dev/null

      - name: Helm upgrade (use GHCR images + resolved tags)
        shell: bash
        env:
          WEB_TAG: ${{ steps.tags.outputs.WEB_TAG }}
          API_TAG: ${{ steps.tags.outputs.API_TAG }}
        run: |
          set -euo pipefail
          echo "Rendering (dry-run) to sanity-check values..."
          helm upgrade --install fullstack "./helm/chart" -n "$NAMESPACE" \
            --dry-run --debug \
            --set web.image.repository="$WEB_IMAGE" \
            --set web.image.tag="$WEB_TAG" \
            --set api.image.repository="$API_IMAGE" \
            --set api.image.tag="$API_TAG" \
            >/dev/null

          echo "Applying release..."
          helm upgrade --install fullstack "./helm/chart" \
            --namespace "$NAMESPACE" --create-namespace \
            --dependency-update \
            --set web.image.repository="$WEB_IMAGE" \
            --set web.image.tag="$WEB_TAG" \
            --set api.image.repository="$API_IMAGE" \
            --set api.image.tag="$API_TAG"

      - name: Wait for rollouts
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "$NAMESPACE" rollout status deploy/api --timeout=300s
          kubectl -n "$NAMESPACE" rollout status deploy/web --timeout=300s
          kubectl -n "$NAMESPACE" get pods -o wide

      - name: Smoke test through Istio
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n istio-system get svc istio-ingressgateway
          kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80 >/tmp/pf.log 2>&1 &
          PF_PID=$!
          if command -v nc >/dev/null 2>&1; then
            for i in {1..20}; do nc -z localhost 8080 && break || sleep 0.5; done
          else
            sleep 3
          fi
          echo "== /api/health =="
          curl -fsS "http://localhost:8080/api/health" | tee /tmp/health.json
          echo "== / (frontend first 10 lines) =="
          curl -fsS "http://localhost:8080/" | head -n 10
          kill "$PF_PID" || true

      - name: If failed, dump diagnostics
        if: failure()
        shell: bash
        run: |
          echo "---- Pods ----"
          kubectl -n "$NAMESPACE" get pods -o wide || true
          echo "---- Events (tail) ----"
          kubectl -n "$NAMESPACE" get events --sort-by=.lastTimestamp | tail -n 100 || true
          echo "---- API logs ----"
          kubectl -n "$NAMESPACE" logs deploy/api -c api --tail=200 || true
          echo "---- WEB logs ----"
          kubectl -n "$NAMESPACE" logs deploy/web -c web --tail=200 || true

